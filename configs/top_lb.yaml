global:
  seed: 42
  data_dir: /kaggle/input/deep-past-initiative-machine-translation
  output_dir: /kaggle/working/outputs/top_lb
  source_column: transliteration
  target_column: translation
  id_column: id

data:
  use_weak_data: true
  weak_max_pairs: 4000
  min_src_tokens: 2
  max_src_tokens: 300
  min_tgt_tokens: 2
  max_tgt_tokens: 220
  ratio_low: 0.12
  ratio_high: 6.5

cv:
  n_splits: 5
  val_fold: 0

training:
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 8
  per_device_eval_batch_size: 16
  max_source_length: 320
  max_target_length: 192
  generation_max_length: 192
  generation_num_beams: 5
  evaluation_strategy: epoch
  save_strategy: epoch
  save_total_limit: 4
  fp16: true
  logging_steps: 40
  early_stopping_patience: 2

models:
  - name: byt5_base_primary
    pretrained_model_name: google/byt5-base
    optimizer: adafactor
    label_smoothing: 0.10
    stages:
      - name: stage1_clean
        learning_rate: 2.0e-4
        warmup_ratio: 0.05
        epochs: 6
        include_weak_data: false
        weak_sample_ratio: 0.0
        hard_replay: false
      - name: stage2_weak
        learning_rate: 1.2e-4
        warmup_ratio: 0.05
        epochs: 4
        include_weak_data: true
        weak_sample_ratio: 0.25
        hard_replay: false
      - name: stage3_hard
        learning_rate: 8.0e-5
        warmup_ratio: 0.03
        epochs: 3
        include_weak_data: true
        weak_sample_ratio: 0.15
        hard_replay: true
        hard_replay_factor: 2

  - name: byt5_small_diversity
    pretrained_model_name: google/byt5-small
    optimizer: adafactor
    label_smoothing: 0.10
    stages:
      - name: stage1_clean
        learning_rate: 2.0e-4
        warmup_ratio: 0.05
        epochs: 8
        include_weak_data: false
        weak_sample_ratio: 0.0
        hard_replay: false
      - name: stage2_weak
        learning_rate: 1.5e-4
        warmup_ratio: 0.05
        epochs: 4
        include_weak_data: true
        weak_sample_ratio: 0.25
        hard_replay: false
      - name: stage3_hard
        learning_rate: 1.0e-4
        warmup_ratio: 0.03
        epochs: 3
        include_weak_data: true
        weak_sample_ratio: 0.15
        hard_replay: true
        hard_replay_factor: 2

  - name: flan_t5_base_secondary
    pretrained_model_name: google/flan-t5-base
    optimizer: adamw_torch
    label_smoothing: 0.10
    stages:
      - name: stage1_clean
        learning_rate: 1.0e-4
        warmup_ratio: 0.05
        epochs: 5
        include_weak_data: false
        weak_sample_ratio: 0.0
        hard_replay: false
      - name: stage2_weak
        learning_rate: 7.0e-5
        warmup_ratio: 0.03
        epochs: 3
        include_weak_data: true
        weak_sample_ratio: 0.20
        hard_replay: true
        hard_replay_factor: 2

inference:
  batch_size: 8
  max_source_length: 320
  max_target_length: 192
  num_beams: 5
  n_best: 5
  enable_entity_repair: true
  ensemble:
    method: consensus_rerank
    rerank_weights:
      bleu: 0.45
      chrf: 0.45
      length: 0.10

hpo:
  phase1_grid:
    learning_rate: [1.0e-4, 2.0e-4, 3.0e-4]
    warmup_ratio: [0.03, 0.05, 0.08]
    beam_size: [3, 4, 5]
    max_target_length: [96, 128, 160]
    label_smoothing: [0.0, 0.05, 0.1]
  phase2_bayesian:
    enabled: true
    n_trials: 35
    objective: maximize_validation_geometric_mean
    seed: 42
